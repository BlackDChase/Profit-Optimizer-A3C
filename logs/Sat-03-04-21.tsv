2021-04-03 02:59:42	INFO	New session initiated
2021-04-03 02:59:42	DEBUG	stateSize = 9
2021-04-03 02:59:42	INFO	GOD inititated
2021-04-03 02:59:42	INFO	Action space: tensor([-12.5000, -10.0000,  -7.5000,  -5.0000,  -2.5000,   0.0000,   2.5000,
          5.0000,   7.5000,  10.0000,  12.5000])
2021-04-03 02:59:42	INFO	Environment inititated
2021-04-03 02:59:42	DEBUG	Boss00 created
2021-04-03 02:59:42	INFO	Environment parsed, Boss inititated
2021-04-03 02:59:42	DEBUG	BOSS 00 training started inside BOSS
2021-04-03 02:59:42	INFO	Starting state=tensor([0.8654, 0.0802, 0.9131, 0.6536, 0.6826, 0.8118, 0.6515, 0.5971, 0.7610])
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0991, 0.0884, 0.0877, 0.0833, 0.1061, 0.0838, 0.0926, 0.0921, 0.0933,
        0.0919, 0.0818], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 4
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0972, 0.0904, 0.0871, 0.0844, 0.1059, 0.0844, 0.0912, 0.0907, 0.0918,
        0.0946, 0.0823], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1022, 0.0867, 0.0848, 0.0803, 0.1054, 0.0836, 0.1003, 0.0910, 0.0982,
        0.0893, 0.0782], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 7
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1022, 0.0870, 0.0862, 0.0826, 0.1047, 0.0847, 0.0968, 0.0914, 0.0959,
        0.0895, 0.0791], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 1
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1025, 0.0846, 0.0859, 0.0851, 0.0973, 0.0834, 0.1018, 0.0903, 0.0984,
        0.0885, 0.0822], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 9
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1062, 0.0857, 0.0862, 0.0814, 0.1054, 0.0844, 0.0992, 0.0865, 0.0969,
        0.0926, 0.0754], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0983, 0.0898, 0.0825, 0.0835, 0.1015, 0.0845, 0.0961, 0.0921, 0.0949,
        0.0924, 0.0844], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 8
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0966, 0.0903, 0.0870, 0.0847, 0.1064, 0.0840, 0.0927, 0.0923, 0.0941,
        0.0916, 0.0803], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 8
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0996, 0.0902, 0.0839, 0.0825, 0.1047, 0.0863, 0.0950, 0.0911, 0.0948,
        0.0923, 0.0796], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1013, 0.0891, 0.0835, 0.0800, 0.1056, 0.0837, 0.0982, 0.0937, 0.0957,
        0.0887, 0.0805], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 9
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0990, 0.0883, 0.0874, 0.0837, 0.1053, 0.0858, 0.0935, 0.0914, 0.0951,
        0.0900, 0.0804], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 7
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0953, 0.0905, 0.0865, 0.0829, 0.1071, 0.0823, 0.0931, 0.0945, 0.0941,
        0.0902, 0.0836], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 10
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1046, 0.0875, 0.0838, 0.0805, 0.1052, 0.0832, 0.0983, 0.0900, 0.0966,
        0.0921, 0.0781], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 10
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1037, 0.0897, 0.0819, 0.0776, 0.1071, 0.0828, 0.0983, 0.0899, 0.0989,
        0.0928, 0.0773], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1013, 0.0887, 0.0831, 0.0844, 0.1031, 0.0850, 0.0966, 0.0896, 0.0972,
        0.0900, 0.0810], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 5
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1023, 0.0906, 0.0827, 0.0815, 0.1041, 0.0853, 0.0933, 0.0890, 0.0942,
        0.0967, 0.0804], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 5
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1021, 0.0908, 0.0812, 0.0799, 0.1069, 0.0830, 0.0934, 0.0920, 0.0968,
        0.0917, 0.0823], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 6
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0977, 0.0929, 0.0826, 0.0824, 0.1051, 0.0859, 0.0916, 0.0912, 0.0943,
        0.0943, 0.0821], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0961, 0.0894, 0.0883, 0.0856, 0.1062, 0.0849, 0.0951, 0.0917, 0.0960,
        0.0872, 0.0796], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 1
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0965, 0.0896, 0.0849, 0.0835, 0.1024, 0.0829, 0.0991, 0.0928, 0.0939,
        0.0908, 0.0836], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 2
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1032, 0.0871, 0.0862, 0.0822, 0.1063, 0.0842, 0.0979, 0.0890, 0.0950,
        0.0916, 0.0773], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 5
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0947, 0.0881, 0.0871, 0.0881, 0.1003, 0.0822, 0.0996, 0.0926, 0.0936,
        0.0866, 0.0871], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 8
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1048, 0.0886, 0.0830, 0.0795, 0.1073, 0.0829, 0.0945, 0.0893, 0.0966,
        0.0953, 0.0782], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1030, 0.0868, 0.0843, 0.0824, 0.1062, 0.0813, 0.0974, 0.0911, 0.0981,
        0.0907, 0.0787], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1012, 0.0911, 0.0825, 0.0817, 0.1043, 0.0852, 0.0925, 0.0893, 0.0937,
        0.0970, 0.0816], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 4
2021-04-03 02:59:42	DEBUG	Action Probability = tensor([0.1061, 0.0972, 0.0910, 0.0870, 0.0885, 0.0814, 0.0949, 0.0941, 0.0825,
        0.0887, 0.0914, 0.0836, 0.0781, 0.1037, 0.0850, 0.0853, 0.0934, 0.0977,
        0.0894, 0.0849, 0.0842, 0.0936, 0.0795, 0.1030, 0.1043],
       grad_fn=<CopySlices>)
2021-04-03 02:59:42	DEBUG	rewards = tensor([ 0.0281,  0.0878,  0.3866, -0.1458,  0.0347, -0.0195,  0.0133, -0.0030,
         0.0655, -0.2030,  0.0043,  0.4769, -0.1075,  0.0013, -0.0037, -0.3955,
         0.0457,  0.0195, -0.0217,  0.1729, -0.1271, -0.0891,  0.5226, -0.2095,
         0.1535])
2021-04-03 02:59:42	DEBUG	Advantage = tensor([ 0.4980,  0.4724,  0.3867, -0.0023,  0.1416,  0.1073,  0.1261,  0.1116,
         0.1155,  0.0462,  0.2482,  0.2423, -0.1901, -0.0853, -0.0888, -0.0894,
         0.3076,  0.2633,  0.2451,  0.2680,  0.0943,  0.2211,  0.3126, -0.2131,
        -0.0014], grad_fn=<CopySlices>)
2021-04-03 02:59:42	INFO	policy loss = 0.3379064202308655
2021-04-03 02:59:42	INFO	critic loss = 0.11519722640514374
2021-04-03 02:59:42	INFO	BOSS 00 episode 0 Completed
2021-04-03 02:59:42	INFO	Starting state=tensor([0.5806, 0.8987, 0.9646, 0.1237, 0.2754, 0.3036, 0.4731, 0.1082, 0.6797])
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1011, 0.0856, 0.0869, 0.0864, 0.1013, 0.0829, 0.0986, 0.0889, 0.0962,
        0.0925, 0.0796], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 7
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0993, 0.0887, 0.0846, 0.0829, 0.1050, 0.0824, 0.0981, 0.0930, 0.0965,
        0.0875, 0.0819], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1033, 0.0860, 0.0860, 0.0841, 0.1013, 0.0853, 0.0983, 0.0885, 0.0973,
        0.0907, 0.0792], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 1
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1023, 0.0881, 0.0860, 0.0800, 0.1064, 0.0849, 0.0984, 0.0909, 0.0969,
        0.0891, 0.0770], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1006, 0.0853, 0.0870, 0.0866, 0.1001, 0.0836, 0.0971, 0.0910, 0.0975,
        0.0891, 0.0819], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 4
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0994, 0.0899, 0.0877, 0.0831, 0.1057, 0.0838, 0.0913, 0.0898, 0.0921,
        0.0967, 0.0806], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 6
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0977, 0.0884, 0.0874, 0.0838, 0.1070, 0.0812, 0.0950, 0.0931, 0.0955,
        0.0894, 0.0815], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 4
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0948, 0.0890, 0.0868, 0.0834, 0.1058, 0.0813, 0.0985, 0.0958, 0.0932,
        0.0870, 0.0843], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 2
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0998, 0.0885, 0.0849, 0.0811, 0.1045, 0.0834, 0.0975, 0.0934, 0.0946,
        0.0902, 0.0821], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0954, 0.0873, 0.0882, 0.0893, 0.1006, 0.0843, 0.0965, 0.0909, 0.0910,
        0.0914, 0.0850], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0973, 0.0903, 0.0885, 0.0818, 0.1072, 0.0852, 0.0919, 0.0922, 0.0941,
        0.0917, 0.0798], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 8
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1012, 0.0877, 0.0854, 0.0810, 0.1064, 0.0824, 0.0982, 0.0930, 0.0974,
        0.0869, 0.0804], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 2
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0964, 0.0894, 0.0874, 0.0838, 0.1069, 0.0818, 0.0953, 0.0944, 0.0947,
        0.0875, 0.0823], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 9
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1007, 0.0867, 0.0863, 0.0819, 0.1038, 0.0855, 0.1016, 0.0887, 0.0978,
        0.0890, 0.0781], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1032, 0.0873, 0.0850, 0.0855, 0.1025, 0.0836, 0.0943, 0.0879, 0.0968,
        0.0940, 0.0799], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 2
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1000, 0.0887, 0.0854, 0.0839, 0.1048, 0.0841, 0.0932, 0.0908, 0.0952,
        0.0926, 0.0813], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 8
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1001, 0.0879, 0.0869, 0.0811, 0.1081, 0.0847, 0.0990, 0.0900, 0.0935,
        0.0914, 0.0772], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 4
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1071, 0.0863, 0.0835, 0.0791, 0.1052, 0.0848, 0.0977, 0.0896, 0.0965,
        0.0913, 0.0788], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1033, 0.0892, 0.0849, 0.0802, 0.1070, 0.0829, 0.0914, 0.0898, 0.0950,
        0.0960, 0.0803], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 6
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0943, 0.0884, 0.0875, 0.0882, 0.0996, 0.0830, 0.0967, 0.0920, 0.0928,
        0.0892, 0.0882], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 3
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1031, 0.0892, 0.0826, 0.0805, 0.1037, 0.0838, 0.0962, 0.0905, 0.0958,
        0.0949, 0.0798], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 7
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1048, 0.0844, 0.0851, 0.0838, 0.1034, 0.0816, 0.0996, 0.0897, 0.0972,
        0.0913, 0.0791], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 0
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0980, 0.0901, 0.0863, 0.0850, 0.1042, 0.0844, 0.0935, 0.0899, 0.0923,
        0.0952, 0.0811], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 8
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.1014, 0.0873, 0.0860, 0.0809, 0.1053, 0.0843, 0.1017, 0.0901, 0.0962,
        0.0891, 0.0776], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 9
2021-04-03 02:59:42	DEBUG	Policy result = tensor([0.0996, 0.0869, 0.0858, 0.0846, 0.1022, 0.0833, 0.0967, 0.0925, 0.0962,
        0.0899, 0.0824], grad_fn=<SoftmaxBackward>)
2021-04-03 02:59:42	INFO	BOSS 00,  Step Done
2021-04-03 02:59:42	DEBUG	Action = 6
2021-04-03 02:59:42	DEBUG	Action Probability = tensor([0.0889, 0.0829, 0.0860, 0.0800, 0.1001, 0.0913, 0.1070, 0.0868, 0.0998,
        0.0954, 0.0941, 0.0854, 0.0875, 0.0819, 0.0850, 0.0952, 0.1081, 0.1071,
        0.0914, 0.0882, 0.0905, 0.1048, 0.0923, 0.0891, 0.0967],
       grad_fn=<CopySlices>)
2021-04-03 02:59:42	DEBUG	rewards = tensor([-0.1334,  0.1006,  0.0986, -0.4401,  0.0371, -0.0822, -0.0178,  0.0264,
         0.4840, -0.4663, -0.0477, -0.0312,  0.0534,  0.0758, -0.1035,  0.2180,
        -0.5820,  0.1889, -0.0375,  0.0677,  0.2781,  0.0205, -0.0637, -0.3157,
         0.4332])
2021-04-03 02:59:42	DEBUG	Advantage = tensor([-0.8062, -0.6024, -0.7110, -0.8208, -0.3869, -0.4294, -0.3295, -0.2021,
        -0.2315, -0.7255, -0.2625, -0.2185, -0.1911, -0.2474, -0.3286, -0.2283,
        -0.4527,  0.1287, -0.0651, -0.0291, -0.0984, -0.3817, -0.4063, -0.3156,
        -0.0013], grad_fn=<CopySlices>)
2021-04-03 02:59:42	INFO	policy loss = -0.800864040851593
2021-04-03 02:59:42	DEBUG	Terminated due to one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [50, 11]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
