2021-04-02 17:44:39	INFO	New session initiated
2021-04-02 17:44:39	DEBUG	stateSize = 9
2021-04-02 17:44:39	DEBUG	kwargs for Policy Net = {'L1': (<class 'torch.nn.modules.linear.Linear'>, 20, Tanh()), 'L2': (<class 'torch.nn.modules.linear.Linear'>, 50, Softmax(dim=0)), 'stateSize': (<class 'torch.nn.modules.linear.Linear'>, 9, ReLU())}
2021-04-02 17:44:39	DEBUG	actionSize is 11
2021-04-02 17:44:39	DEBUG	stateSize is 9
2021-04-02 17:44:39	DEBUG	Layers for Policy Net = [Linear(in_features=9, out_features=20, bias=True), ReLU(), Linear(in_features=20, out_features=50, bias=True), Tanh(), Linear(in_features=50, out_features=11, bias=True), Softmax(dim=0)]
2021-04-02 17:44:39	DEBUG	kwargs for Critic Net = {'L1': (<class 'torch.nn.modules.linear.Linear'>, 30, ReLU6()), 'L2': (<class 'torch.nn.modules.linear.Linear'>, 40, ReLU6()), 'stateSize': (<class 'torch.nn.modules.linear.Linear'>, 9, ReLU())}
2021-04-02 17:44:39	DEBUG	actionSize is 1
2021-04-02 17:44:39	DEBUG	stateSize is 9
2021-04-02 17:44:39	DEBUG	Layers for Critic Net = [Linear(in_features=9, out_features=30, bias=True), ReLU(), Linear(in_features=30, out_features=40, bias=True), ReLU6(), Linear(in_features=40, out_features=1, bias=True), ReLU6()]
2021-04-02 17:44:39	INFO	GOD inititated
2021-04-02 17:44:39	INFO	Action space: tensor([-12.5000, -10.0000,  -7.5000,  -5.0000,  -2.5000,   0.0000,   2.5000,
          5.0000,   7.5000,  10.0000,  12.5000])
2021-04-02 17:44:39	INFO	Environment inititated
2021-04-02 17:44:39	DEBUG	kwargs for Policy Net = {'L1': (<class 'torch.nn.modules.linear.Linear'>, 20, Tanh()), 'L2': (<class 'torch.nn.modules.linear.Linear'>, 50, Softmax(dim=0)), 'stateSize': (<class 'torch.nn.modules.linear.Linear'>, 9, ReLU())}
2021-04-02 17:44:39	DEBUG	actionSize is 11
2021-04-02 17:44:39	DEBUG	stateSize is 9
2021-04-02 17:44:39	DEBUG	Layers for Policy Net = [Linear(in_features=9, out_features=20, bias=True), ReLU(), Linear(in_features=20, out_features=50, bias=True), Tanh(), Linear(in_features=50, out_features=11, bias=True), Softmax(dim=0)]
2021-04-02 17:44:39	DEBUG	kwargs for Critic Net = {'L1': (<class 'torch.nn.modules.linear.Linear'>, 30, ReLU6()), 'L2': (<class 'torch.nn.modules.linear.Linear'>, 40, ReLU6()), 'stateSize': (<class 'torch.nn.modules.linear.Linear'>, 9, ReLU())}
2021-04-02 17:44:39	DEBUG	actionSize is 1
2021-04-02 17:44:39	DEBUG	stateSize is 9
2021-04-02 17:44:39	DEBUG	Layers for Critic Net = [Linear(in_features=9, out_features=30, bias=True), ReLU(), Linear(in_features=30, out_features=40, bias=True), ReLU6(), Linear(in_features=40, out_features=1, bias=True), ReLU6()]
2021-04-02 17:44:39	DEBUG	Boss00 created
2021-04-02 17:44:39	INFO	Environment parsed, Boss inititated
2021-04-02 17:44:39	DEBUG	Boss00 training started via GOD
2021-04-02 17:44:39	DEBUG	BOSS 00 training started inside BOSS
2021-04-02 17:44:39	INFO	Starting state=tensor([0.3332, 0.5755, 0.3014, 0.2707, 0.1807, 0.3660, 0.8756, 0.7361, 0.5279])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.3332, 0.5755, 0.3014, 0.2707, 0.1807, 0.3660, 0.8756, 0.7361, 0.5279])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0931, 0.0831, 0.0838, 0.0802, 0.0887, 0.0855, 0.0793, 0.1071, 0.1211,
        0.1053, 0.0729], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 1
2021-04-02 17:44:39	DEBUG	current state : tensor([0.8932, 0.6079, 0.2471, 0.3124, 0.8265, 0.5498, 0.5282, 0.8161, 0.5357])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0952, 0.0783, 0.0847, 0.0767, 0.0856, 0.0820, 0.0777, 0.1092, 0.1314,
        0.1109, 0.0683], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 2
2021-04-02 17:44:39	DEBUG	current state : tensor([0.2581, 0.0713, 0.9000, 0.0242, 0.8675, 0.9380, 0.8562, 0.0187, 0.4416])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0892, 0.0778, 0.0817, 0.0834, 0.0864, 0.0875, 0.0797, 0.1103, 0.1211,
        0.1103, 0.0726], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 5
2021-04-02 17:44:39	DEBUG	current state : tensor([0.3429, 0.5616, 0.2872, 0.6589, 0.0280, 0.1045, 0.7463, 0.4112, 0.0629])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0940, 0.0863, 0.0826, 0.0822, 0.0897, 0.0866, 0.0800, 0.1067, 0.1142,
        0.1005, 0.0773], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 7
2021-04-02 17:44:39	DEBUG	current state : tensor([0.2907, 0.8859, 0.9981, 0.6537, 0.7666, 0.5456, 0.8146, 0.4515, 0.8074])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0930, 0.0767, 0.0853, 0.0772, 0.0856, 0.0825, 0.0788, 0.1077, 0.1373,
        0.1102, 0.0657], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 3
2021-04-02 17:44:39	DEBUG	current state : tensor([0.0973, 0.8437, 0.8351, 0.7159, 0.1257, 0.5352, 0.2969, 0.1568, 0.0383])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0963, 0.0818, 0.0828, 0.0815, 0.0874, 0.0851, 0.0800, 0.1086, 0.1193,
        0.1003, 0.0770], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 4
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4074, 0.6709, 0.5067, 0.7948, 0.8064, 0.4928, 0.8873, 0.3209, 0.7428])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0944, 0.0795, 0.0844, 0.0781, 0.0867, 0.0831, 0.0792, 0.1078, 0.1304,
        0.1077, 0.0686], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 5
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4043, 0.9219, 0.9049, 0.9583, 0.5923, 0.0463, 0.7885, 0.9879, 0.2970])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0917, 0.0785, 0.0868, 0.0750, 0.0837, 0.0822, 0.0779, 0.1073, 0.1437,
        0.1077, 0.0655], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 5
2021-04-02 17:44:39	DEBUG	current state : tensor([0.5118, 0.2195, 0.4063, 0.3506, 0.3358, 0.7779, 0.0426, 0.1093, 0.4791])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0954, 0.0785, 0.0800, 0.0871, 0.0899, 0.0879, 0.0826, 0.1078, 0.1122,
        0.1020, 0.0766], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 2
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4067, 0.2468, 0.3537, 0.8293, 0.2082, 0.3244, 0.3325, 0.8513, 0.3108])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0921, 0.0815, 0.0833, 0.0827, 0.0899, 0.0873, 0.0816, 0.1049, 0.1229,
        0.1007, 0.0729], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 8
2021-04-02 17:44:39	DEBUG	current state : tensor([0.8374, 0.8336, 0.1062, 0.1619, 0.1163, 0.2159, 0.6272, 0.9607, 0.1166])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0947, 0.0833, 0.0836, 0.0781, 0.0873, 0.0838, 0.0779, 0.1082, 0.1239,
        0.1074, 0.0719], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 8
2021-04-02 17:44:39	DEBUG	current state : tensor([0.2657, 0.2248, 0.1410, 0.0623, 0.9577, 0.6259, 0.6769, 0.0517, 0.2627])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0911, 0.0813, 0.0835, 0.0819, 0.0888, 0.0872, 0.0789, 0.1072, 0.1190,
        0.1070, 0.0741], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 5
2021-04-02 17:44:39	DEBUG	current state : tensor([1.0138e-01, 6.0015e-01, 1.4120e-04, 6.5811e-02, 4.0909e-01, 7.9605e-01,
        9.8749e-01, 4.7970e-01, 1.2078e-01])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0920, 0.0843, 0.0847, 0.0805, 0.0896, 0.0865, 0.0781, 0.1058, 0.1190,
        0.1057, 0.0738], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 8
2021-04-02 17:44:39	DEBUG	current state : tensor([0.7055, 0.6724, 0.9052, 0.7862, 0.8553, 0.0806, 0.1642, 0.7170, 0.3915])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0924, 0.0771, 0.0848, 0.0779, 0.0850, 0.0833, 0.0802, 0.1083, 0.1401,
        0.1043, 0.0666], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 8
2021-04-02 17:44:39	DEBUG	current state : tensor([0.7736, 0.3591, 0.0781, 0.1953, 0.9190, 0.7420, 0.9593, 0.3133, 0.6212])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0939, 0.0796, 0.0837, 0.0786, 0.0866, 0.0836, 0.0787, 0.1093, 0.1266,
        0.1090, 0.0705], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 4
2021-04-02 17:44:39	DEBUG	current state : tensor([0.0093, 0.7158, 0.5324, 0.7472, 0.0690, 0.9947, 0.8875, 0.3653, 0.5421])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0955, 0.0825, 0.0828, 0.0813, 0.0893, 0.0853, 0.0800, 0.1071, 0.1185,
        0.1037, 0.0741], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 0
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4808, 0.6161, 0.2646, 0.5838, 0.3291, 0.4163, 0.8799, 0.7829, 0.8093])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0948, 0.0813, 0.0840, 0.0789, 0.0877, 0.0838, 0.0797, 0.1073, 0.1265,
        0.1056, 0.0703], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 2
2021-04-02 17:44:39	DEBUG	current state : tensor([0.1484, 0.6963, 0.5863, 0.8782, 0.4609, 0.1599, 0.2653, 0.3388, 0.9649])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0938, 0.0775, 0.0847, 0.0811, 0.0871, 0.0846, 0.0817, 0.1062, 0.1326,
        0.1004, 0.0703], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 3
2021-04-02 17:44:39	DEBUG	current state : tensor([0.1614, 0.4444, 0.5504, 0.3465, 0.2324, 0.0345, 0.0177, 0.0142, 0.8729])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0937, 0.0792, 0.0812, 0.0870, 0.0898, 0.0879, 0.0840, 0.1066, 0.1174,
        0.0969, 0.0764], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 9
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4295, 0.5801, 0.0508, 0.7311, 0.1602, 0.1523, 0.2219, 0.9209, 0.6969])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0947, 0.0807, 0.0842, 0.0804, 0.0879, 0.0850, 0.0804, 0.1064, 0.1270,
        0.1010, 0.0724], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 5
2021-04-02 17:44:39	DEBUG	current state : tensor([0.3572, 0.6383, 0.8909, 0.2894, 0.1357, 0.4557, 0.0725, 0.0515, 0.4415])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0960, 0.0790, 0.0808, 0.0833, 0.0883, 0.0863, 0.0820, 0.1091, 0.1172,
        0.1005, 0.0774], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 9
2021-04-02 17:44:39	DEBUG	current state : tensor([0.2876, 0.2463, 0.0078, 0.7788, 0.6546, 0.0603, 0.6098, 0.6017, 0.0863])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0911, 0.0853, 0.0820, 0.0814, 0.0888, 0.0873, 0.0803, 0.1064, 0.1208,
        0.1026, 0.0740], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 2
2021-04-02 17:44:39	DEBUG	current state : tensor([0.5112, 0.8241, 0.1528, 0.8561, 0.6676, 0.8283, 0.2356, 0.2709, 0.3038])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0960, 0.0814, 0.0838, 0.0799, 0.0881, 0.0831, 0.0800, 0.1080, 0.1238,
        0.1048, 0.0711], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 1
2021-04-02 17:44:39	DEBUG	current state : tensor([0.8094, 0.3425, 0.4762, 0.0873, 0.8518, 0.3283, 0.4384, 0.2391, 0.9890])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0938, 0.0771, 0.0822, 0.0822, 0.0866, 0.0854, 0.0801, 0.1095, 0.1243,
        0.1085, 0.0703], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 6
2021-04-02 17:44:39	DEBUG	current state : tensor([0.9160, 0.8507, 0.9343, 0.0262, 0.2526, 0.5614, 0.5524, 0.8296, 0.4750])
2021-04-02 17:44:39	DEBUG	Policy result = tensor([0.0952, 0.0774, 0.0838, 0.0766, 0.0844, 0.0817, 0.0790, 0.1108, 0.1319,
        0.1100, 0.0693], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:39	INFO	BOSS 00,  Step Done
2021-04-02 17:44:39	DEBUG	Action = 7
2021-04-02 17:44:39	DEBUG	Action Probability = tensor([0.0831, 0.0847, 0.0875, 0.1067, 0.0772, 0.0874, 0.0831, 0.0822, 0.0800,
        0.1229, 0.1239, 0.0872, 0.1190, 0.1401, 0.0866, 0.0955, 0.0840, 0.0811,
        0.0969, 0.0850, 0.1005, 0.0820, 0.0814, 0.0801, 0.1108],
       grad_fn=<CopySlices>)
2021-04-02 17:44:39	DEBUG	rewards = tensor([ 0.0289, -0.1385,  0.1681,  0.0943, -0.0041, -0.0704,  0.1015, -0.3595,
         0.0111,  0.4914, -0.1617,  0.0381,  0.0510, -0.2424,  0.0033, -0.0479,
         0.0119, -0.0406,  0.0583,  0.0208, -0.1127,  0.2954, -0.3898,  0.4655,
        -0.5138])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.3332, 0.5755, 0.3014, 0.2707, 0.1807, 0.3660, 0.8756, 0.7361, 0.5279])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.8932, 0.6079, 0.2471, 0.3124, 0.8265, 0.5498, 0.5282, 0.8161, 0.5357])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.2581, 0.0713, 0.9000, 0.0242, 0.8675, 0.9380, 0.8562, 0.0187, 0.4416])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.3429, 0.5616, 0.2872, 0.6589, 0.0280, 0.1045, 0.7463, 0.4112, 0.0629])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.2907, 0.8859, 0.9981, 0.6537, 0.7666, 0.5456, 0.8146, 0.4515, 0.8074])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.0973, 0.8437, 0.8351, 0.7159, 0.1257, 0.5352, 0.2969, 0.1568, 0.0383])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4074, 0.6709, 0.5067, 0.7948, 0.8064, 0.4928, 0.8873, 0.3209, 0.7428])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.4043, 0.9219, 0.9049, 0.9583, 0.5923, 0.0463, 0.7885, 0.9879, 0.2970])
2021-04-02 17:44:39	DEBUG	current state : tensor([0.5118, 0.2195, 0.4063, 0.3506, 0.3358, 0.7779, 0.0426, 0.1093, 0.4791])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4067, 0.2468, 0.3537, 0.8293, 0.2082, 0.3244, 0.3325, 0.8513, 0.3108])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.8374, 0.8336, 0.1062, 0.1619, 0.1163, 0.2159, 0.6272, 0.9607, 0.1166])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.2657, 0.2248, 0.1410, 0.0623, 0.9577, 0.6259, 0.6769, 0.0517, 0.2627])
2021-04-02 17:44:40	DEBUG	current state : tensor([1.0138e-01, 6.0015e-01, 1.4120e-04, 6.5811e-02, 4.0909e-01, 7.9605e-01,
        9.8749e-01, 4.7970e-01, 1.2078e-01])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7055, 0.6724, 0.9052, 0.7862, 0.8553, 0.0806, 0.1642, 0.7170, 0.3915])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7736, 0.3591, 0.0781, 0.1953, 0.9190, 0.7420, 0.9593, 0.3133, 0.6212])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.0093, 0.7158, 0.5324, 0.7472, 0.0690, 0.9947, 0.8875, 0.3653, 0.5421])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4808, 0.6161, 0.2646, 0.5838, 0.3291, 0.4163, 0.8799, 0.7829, 0.8093])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.1484, 0.6963, 0.5863, 0.8782, 0.4609, 0.1599, 0.2653, 0.3388, 0.9649])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.1614, 0.4444, 0.5504, 0.3465, 0.2324, 0.0345, 0.0177, 0.0142, 0.8729])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4295, 0.5801, 0.0508, 0.7311, 0.1602, 0.1523, 0.2219, 0.9209, 0.6969])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3572, 0.6383, 0.8909, 0.2894, 0.1357, 0.4557, 0.0725, 0.0515, 0.4415])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.2876, 0.2463, 0.0078, 0.7788, 0.6546, 0.0603, 0.6098, 0.6017, 0.0863])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5112, 0.8241, 0.1528, 0.8561, 0.6676, 0.8283, 0.2356, 0.2709, 0.3038])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.8094, 0.3425, 0.4762, 0.0873, 0.8518, 0.3283, 0.4384, 0.2391, 0.9890])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9160, 0.8507, 0.9343, 0.0262, 0.2526, 0.5614, 0.5524, 0.8296, 0.4750])
2021-04-02 17:44:40	DEBUG	Advantage = tensor([-3.2404, -3.1116, -2.8399, -2.8623, -2.8176, -2.6647, -2.4385, -2.4046,
        -1.9058, -1.7419, -2.0662, -1.7698, -1.6675, -1.5919, -1.2035, -1.0569,
        -0.8226, -0.6577, -0.4463, -0.3272, -0.1792,  0.1176, -0.0458,  0.4932,
         0.1830], grad_fn=<CopySlices>)
2021-04-02 17:44:40	INFO	policy loss = -3.527231216430664
2021-04-02 17:44:40	INFO	critic loss = 0.20110373198986053
2021-04-02 17:44:40	INFO	BOSS 00 episode 0 Completed
2021-04-02 17:44:40	INFO	Starting state=tensor([0.3732, 0.0878, 0.9943, 0.1143, 0.4684, 0.5998, 0.3827, 0.2941, 0.1650])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3732, 0.0878, 0.9943, 0.1143, 0.4684, 0.5998, 0.3827, 0.2941, 0.1650])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0886, 0.0787, 0.0821, 0.0842, 0.0887, 0.0884, 0.0815, 0.1078, 0.1197,
        0.1061, 0.0741], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 1
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4782, 0.4963, 0.6842, 0.0126, 0.0792, 0.2172, 0.3061, 0.4114, 0.1494])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0923, 0.0818, 0.0814, 0.0832, 0.0897, 0.0880, 0.0807, 0.1079, 0.1154,
        0.1030, 0.0767], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 10
2021-04-02 17:44:40	DEBUG	current state : tensor([0.8840, 0.7508, 0.2645, 0.5688, 0.7331, 0.5433, 0.0471, 0.2085, 0.9831])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0958, 0.0781, 0.0819, 0.0801, 0.0868, 0.0820, 0.0816, 0.1110, 0.1269,
        0.1059, 0.0698], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 5
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7969, 0.2247, 0.0805, 0.5646, 0.7733, 0.2775, 0.1959, 0.7128, 0.6534])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0940, 0.0788, 0.0830, 0.0809, 0.0884, 0.0851, 0.0809, 0.1068, 0.1266,
        0.1049, 0.0705], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 4
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7563, 0.6576, 0.0879, 0.8465, 0.4918, 0.6269, 0.4496, 0.1626, 0.8171])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0952, 0.0806, 0.0821, 0.0802, 0.0878, 0.0835, 0.0811, 0.1097, 0.1232,
        0.1047, 0.0718], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 6
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4138, 0.9280, 0.7859, 0.2863, 0.2371, 0.5616, 0.0588, 0.4425, 0.8101])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0978, 0.0766, 0.0834, 0.0794, 0.0860, 0.0824, 0.0805, 0.1092, 0.1270,
        0.1055, 0.0722], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 1
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5015, 0.5899, 0.1344, 0.9216, 0.9569, 0.2746, 0.1960, 0.7936, 0.7118])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0943, 0.0790, 0.0846, 0.0786, 0.0870, 0.0830, 0.0804, 0.1063, 0.1344,
        0.1055, 0.0670], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 9
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7247, 0.5466, 0.6862, 0.8850, 0.2864, 0.3739, 0.4180, 0.2734, 0.8119])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0945, 0.0783, 0.0819, 0.0811, 0.0873, 0.0843, 0.0819, 0.1095, 0.1260,
        0.1032, 0.0720], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 0
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4175, 0.2529, 0.9124, 0.5599, 0.1104, 0.3233, 0.2808, 0.7771, 0.2514])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0903, 0.0795, 0.0839, 0.0823, 0.0884, 0.0868, 0.0826, 0.1057, 0.1261,
        0.1020, 0.0724], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 2
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3310, 0.8276, 0.4980, 0.7190, 0.8495, 0.1398, 0.9750, 0.1032, 0.3660])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0932, 0.0832, 0.0846, 0.0783, 0.0868, 0.0833, 0.0789, 0.1081, 0.1265,
        0.1069, 0.0702], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 8
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5449, 0.6277, 0.1003, 0.1452, 0.4829, 0.5604, 0.7021, 0.7328, 0.8272])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0956, 0.0800, 0.0840, 0.0789, 0.0874, 0.0835, 0.0788, 0.1080, 0.1256,
        0.1075, 0.0707], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 5
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4046, 0.2559, 0.5046, 0.7264, 0.3408, 0.1662, 0.7740, 0.5244, 0.7202])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0909, 0.0811, 0.0831, 0.0826, 0.0898, 0.0871, 0.0824, 0.1054, 0.1244,
        0.1019, 0.0713], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 2
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5182, 0.5940, 0.4243, 0.3888, 0.8147, 0.3580, 0.5171, 0.7909, 0.0897])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0918, 0.0804, 0.0851, 0.0776, 0.0864, 0.0842, 0.0775, 0.1077, 0.1297,
        0.1103, 0.0694], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 4
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7183, 0.6598, 0.0401, 0.6343, 0.1211, 0.3379, 0.6126, 0.6915, 0.9679])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0951, 0.0818, 0.0826, 0.0795, 0.0880, 0.0838, 0.0804, 0.1090, 0.1229,
        0.1048, 0.0720], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 6
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3401, 0.5807, 0.2031, 0.1144, 0.2849, 0.3624, 0.0881, 0.7807, 0.4075])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0943, 0.0808, 0.0838, 0.0819, 0.0898, 0.0861, 0.0797, 0.1057, 0.1195,
        0.1046, 0.0737], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 4
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9779, 0.5071, 0.1055, 0.3412, 0.7840, 0.2373, 0.3368, 0.7052, 0.1359])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0943, 0.0820, 0.0828, 0.0789, 0.0865, 0.0831, 0.0793, 0.1099, 0.1248,
        0.1071, 0.0714], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 3
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9826, 0.1892, 0.8920, 0.1845, 0.5637, 0.6265, 0.1934, 0.9544, 0.1928])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0916, 0.0769, 0.0838, 0.0785, 0.0852, 0.0837, 0.0803, 0.1096, 0.1314,
        0.1091, 0.0699], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 0
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4432, 0.9993, 0.4447, 0.0355, 0.0544, 0.1657, 0.2218, 0.5622, 0.9658])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0967, 0.0784, 0.0833, 0.0803, 0.0874, 0.0838, 0.0799, 0.1086, 0.1244,
        0.1044, 0.0729], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 7
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5508, 0.6549, 0.0573, 0.9532, 0.9987, 0.8029, 0.6468, 0.9222, 0.8232])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0963, 0.0784, 0.0847, 0.0752, 0.0847, 0.0807, 0.0779, 0.1086, 0.1370,
        0.1103, 0.0661], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 7
2021-04-02 17:44:40	DEBUG	current state : tensor([0.0350, 0.9333, 0.3228, 0.5533, 0.8055, 0.5822, 0.7088, 0.4806, 0.6489])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0951, 0.0800, 0.0861, 0.0773, 0.0870, 0.0830, 0.0775, 0.1063, 0.1299,
        0.1090, 0.0687], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 0
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9494, 0.4351, 0.1568, 0.4630, 0.8298, 0.4612, 0.0023, 0.3209, 0.9121])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0957, 0.0782, 0.0818, 0.0814, 0.0873, 0.0835, 0.0813, 0.1102, 0.1237,
        0.1058, 0.0710], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 5
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9629, 0.5898, 0.0341, 0.3653, 0.7630, 0.3648, 0.2665, 0.9354, 0.5537])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0967, 0.0790, 0.0836, 0.0774, 0.0860, 0.0821, 0.0783, 0.1092, 0.1291,
        0.1092, 0.0695], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 6
2021-04-02 17:44:40	DEBUG	current state : tensor([0.0510, 0.7925, 0.1669, 0.8784, 0.5946, 0.7125, 0.6110, 0.0226, 0.0569])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0957, 0.0849, 0.0838, 0.0813, 0.0897, 0.0850, 0.0796, 0.1059, 0.1172,
        0.1026, 0.0743], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 2
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5794, 0.1615, 0.3714, 0.3843, 0.2926, 0.2185, 0.9764, 0.1127, 0.5198])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0918, 0.0837, 0.0808, 0.0832, 0.0895, 0.0872, 0.0814, 0.1087, 0.1146,
        0.1028, 0.0765], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 4
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9372, 0.7973, 0.8792, 0.3403, 0.6627, 0.9899, 0.5066, 0.9829, 0.0204])
2021-04-02 17:44:40	DEBUG	Policy result = tensor([0.0945, 0.0768, 0.0861, 0.0741, 0.0828, 0.0800, 0.0770, 0.1102, 0.1378,
        0.1140, 0.0666], grad_fn=<SoftmaxBackward>)
2021-04-02 17:44:40	INFO	BOSS 00,  Step Done
2021-04-02 17:44:40	DEBUG	Action = 1
2021-04-02 17:44:40	DEBUG	Action Probability = tensor([0.0787, 0.0767, 0.0820, 0.0884, 0.0811, 0.0766, 0.1055, 0.0945, 0.0839,
        0.1265, 0.0835, 0.0831, 0.0864, 0.0804, 0.0898, 0.0789, 0.0916, 0.1086,
        0.1086, 0.0951, 0.0835, 0.0783, 0.0838, 0.0895, 0.0768],
       grad_fn=<CopySlices>)
2021-04-02 17:44:40	DEBUG	rewards = tensor([ 0.1953,  0.2249, -0.4193,  0.3274,  0.1119, -0.1695, -0.0314, -0.1226,
         0.1903, -0.1090, -0.1504,  0.1752,  0.0473, -0.0269, -0.0719, -0.3124,
         0.3590, -0.1897,  0.0097, -0.4730,  0.1490,  0.0103, -0.3655,  0.5958,
        -0.3941])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3732, 0.0878, 0.9943, 0.1143, 0.4684, 0.5998, 0.3827, 0.2941, 0.1650])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4782, 0.4963, 0.6842, 0.0126, 0.0792, 0.2172, 0.3061, 0.4114, 0.1494])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.8840, 0.7508, 0.2645, 0.5688, 0.7331, 0.5433, 0.0471, 0.2085, 0.9831])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7969, 0.2247, 0.0805, 0.5646, 0.7733, 0.2775, 0.1959, 0.7128, 0.6534])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7563, 0.6576, 0.0879, 0.8465, 0.4918, 0.6269, 0.4496, 0.1626, 0.8171])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4138, 0.9280, 0.7859, 0.2863, 0.2371, 0.5616, 0.0588, 0.4425, 0.8101])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5015, 0.5899, 0.1344, 0.9216, 0.9569, 0.2746, 0.1960, 0.7936, 0.7118])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7247, 0.5466, 0.6862, 0.8850, 0.2864, 0.3739, 0.4180, 0.2734, 0.8119])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4175, 0.2529, 0.9124, 0.5599, 0.1104, 0.3233, 0.2808, 0.7771, 0.2514])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3310, 0.8276, 0.4980, 0.7190, 0.8495, 0.1398, 0.9750, 0.1032, 0.3660])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5449, 0.6277, 0.1003, 0.1452, 0.4829, 0.5604, 0.7021, 0.7328, 0.8272])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4046, 0.2559, 0.5046, 0.7264, 0.3408, 0.1662, 0.7740, 0.5244, 0.7202])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5182, 0.5940, 0.4243, 0.3888, 0.8147, 0.3580, 0.5171, 0.7909, 0.0897])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.7183, 0.6598, 0.0401, 0.6343, 0.1211, 0.3379, 0.6126, 0.6915, 0.9679])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.3401, 0.5807, 0.2031, 0.1144, 0.2849, 0.3624, 0.0881, 0.7807, 0.4075])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9779, 0.5071, 0.1055, 0.3412, 0.7840, 0.2373, 0.3368, 0.7052, 0.1359])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9826, 0.1892, 0.8920, 0.1845, 0.5637, 0.6265, 0.1934, 0.9544, 0.1928])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.4432, 0.9993, 0.4447, 0.0355, 0.0544, 0.1657, 0.2218, 0.5622, 0.9658])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5508, 0.6549, 0.0573, 0.9532, 0.9987, 0.8029, 0.6468, 0.9222, 0.8232])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.0350, 0.9333, 0.3228, 0.5533, 0.8055, 0.5822, 0.7088, 0.4806, 0.6489])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9494, 0.4351, 0.1568, 0.4630, 0.8298, 0.4612, 0.0023, 0.3209, 0.9121])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9629, 0.5898, 0.0341, 0.3653, 0.7630, 0.3648, 0.2665, 0.9354, 0.5537])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.0510, 0.7925, 0.1669, 0.8784, 0.5946, 0.7125, 0.6110, 0.0226, 0.0569])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.5794, 0.1615, 0.3714, 0.3843, 0.2926, 0.2185, 0.9764, 0.1127, 0.5198])
2021-04-02 17:44:40	DEBUG	current state : tensor([0.9372, 0.7973, 0.8792, 0.3403, 0.6627, 0.9899, 0.5066, 0.9829, 0.0204])
2021-04-02 17:44:40	DEBUG	Advantage = tensor([-4.5292, -4.5338, -4.5800, -3.9943, -4.1484, -4.0749, -3.7174, -3.5291,
        -3.1983, -3.1697, -2.9013, -2.5471, -2.5132, -2.3847, -2.1392, -1.8598,
        -1.3653, -1.4974, -1.0893, -0.8951, -0.2167, -0.1599,  0.0376,  0.5938,
         0.2256], grad_fn=<CopySlices>)
2021-04-02 17:44:40	INFO	policy loss = -5.675750732421875
